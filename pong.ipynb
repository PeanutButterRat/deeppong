{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pong import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_dataset(save_to_disk=False):\n",
    "    '''Generates a dataset of frames and states from Pong.'''\n",
    "    pong = Pong(sound_enabled=False)\n",
    "    df = pd.DataFrame(columns=['left_x', 'left_y', 'right_x', 'right_y', 'ball_x', 'ball_y', 'ball_vx', 'ball_vy', 'left_score', 'right_score'])\n",
    "\n",
    "    screenshots = []\n",
    "    i = 0\n",
    "\n",
    "    # Make sure the image directory exists.\n",
    "    if save_to_disk:\n",
    "        os.makedirs(DATA_DIRECTORY_NAME, exist_ok=True)\n",
    "\n",
    "    for p1_score, p2_score in SCORE_COMBINATIONS:\n",
    "        for state in range(STATES_PER_SCORE):\n",
    "            pong.restart()  # Start a new phase of the game.\n",
    "\n",
    "            # Generate a random state.\n",
    "            pong.p1.y = random.uniform(PADDLE_HEIGHT, SCREEN_HEIGHT)\n",
    "            pong.p1.score = p1_score\n",
    "\n",
    "            pong.p2.y = random.uniform(PADDLE_HEIGHT, SCREEN_HEIGHT)\n",
    "            pong.p2.score = p2_score\n",
    "\n",
    "            pong.ball.x = random.uniform(GOAL_PADDING, SCREEN_WIDTH - GOAL_PADDING)\n",
    "            pong.ball.y = random.uniform(BALL_SIZE, SCREEN_HEIGHT)\n",
    "\n",
    "            overrides = {}\n",
    "            pong.paused = False\n",
    "\n",
    "            # Simulate the game for a couple timesteps.\n",
    "            for _ in range(TIMESTEPS):\n",
    "                pong.update(IDEAL_DT, overrides)\n",
    "                state, screenshot = pong.capture()\n",
    "\n",
    "                if save_to_disk:\n",
    "                    pygame.image.save(screenshot, f'{DATA_DIRECTORY_NAME}/{i}.png')\n",
    "\n",
    "                # Create a new record.\n",
    "                df.loc[len(df)] = state\n",
    "                screenshot = pygame.surfarray.array3d(screenshot).astype(np.uint8)\n",
    "                screenshot = np.transpose(screenshot, (1, 0, 2))  # (height, width, 3)\n",
    "                screenshot = screenshot[..., 0] / 255.0 # or 1 or 2, since R = G = B\n",
    "                screenshot = screenshot.reshape(-1)\n",
    "                screenshots.append(screenshot)\n",
    "\n",
    "                # Simulate player input that simply chases the ball.\n",
    "                overrides = {}\n",
    "\n",
    "                for paddle in pong.paddles:                \n",
    "                    if pong.ball.y >= paddle.y :  # Ball is above the paddle, move up.\n",
    "                        overrides[paddle.up] = True\n",
    "                    elif pong.ball.y - BALL_SIZE <= paddle.y - PADDLE_HEIGHT:  # Ball is below the paddle, move down\n",
    "                        overrides[paddle.down] = True\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    if save_to_disk:\n",
    "        df.to_csv(SPREADSHEET_FILEPATH, index=False)\n",
    "\n",
    "    # Convert to numpy arrays.\n",
    "    states = df.to_numpy()\n",
    "    screenshots = np.stack(screenshots)\n",
    "\n",
    "    return states, screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_dataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    '''Loads in a previously generated dataset from disk.'''\n",
    "    # Read in the state data.\n",
    "    df = pd.read_csv(SPREADSHEET_FILEPATH)\n",
    "\n",
    "    # Load in all the screenshots.\n",
    "    screenshots = []\n",
    "\n",
    "    for i, _ in df.iterrows():\n",
    "        path = f'{DATA_DIRECTORY_NAME}/{i}.png'\n",
    "        screenshot = tf.io.read_file(path)\n",
    "        screenshot = tf.image.decode_png(screenshot, channels=1).numpy() / 255\n",
    "        screenshot = tf.squeeze(screenshot, axis=-1)\n",
    "        screenshot = tf.reshape(screenshot, [-1])\n",
    "        screenshots.append(screenshot)\n",
    "\n",
    "    # Convert to numpy arrays.\n",
    "    states = df.to_numpy()\n",
    "    screenshots = np.stack(screenshots)\n",
    "\n",
    "    return states, screenshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def generate_configs(options):\n",
    "    '''Generates the Cartesian product of the supplied dictionary.'''\n",
    "    keys = options.keys()\n",
    "    values = options.values()\n",
    "    return [dict(zip(keys, combo)) for combo in product(*values)]\n",
    "\n",
    "\n",
    "def stringify(config):\n",
    "    '''Converts a model config to a single string based its values.'''\n",
    "    return '_'.join([str(option).replace(' ','') for option in config.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26288547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "\n",
    "PIXELS = SCREEN_WIDTH * SCREEN_HEIGHT\n",
    "\n",
    "\n",
    "def generate_fcnn_models(options):\n",
    "    '''Generates a list of compiled feed-forward neural networks.'''\n",
    "    models = []\n",
    "\n",
    "    for config in generate_configs(options):\n",
    "        layers = []\n",
    "\n",
    "        # Add the hidden layers.\n",
    "        for neurons in config['neurons']:\n",
    "            layers.append(Dense(neurons, activation=config['activation']))\n",
    "\n",
    "        # Add the output layer.\n",
    "        layers.append(Dense(PIXELS, activation='sigmoid'))\n",
    "\n",
    "        model = Sequential(layers)\n",
    "        model.compile(optimizer=config['optimizer'], loss='binary_crossentropy')\n",
    "        name = 'dense_' + stringify(config)\n",
    "\n",
    "        models.append((model, name))\n",
    "\n",
    "    return models[:3]\n",
    "\n",
    "\n",
    "def generate_rnn_models(options):\n",
    "    '''Generates a list of compiled recurrent neural networks.'''\n",
    "    models = []\n",
    "\n",
    "    for config in generate_configs(options):\n",
    "        layers = []\n",
    "\n",
    "        # LSTM layers.\n",
    "        for i, neurons in enumerate(config['bottom_layers'], 1):\n",
    "            return_sequences = i != len(config['bottom_layers'])\n",
    "            layers.append(LSTM(neurons, activation='tanh', dropout=config['dropout'], return_sequences=return_sequences))\n",
    "\n",
    "        # Decoder layers.\n",
    "        for neurons in config['top_layers']:\n",
    "            layers.append(Dense(neurons, activation='relu'))\n",
    "\n",
    "        # Output layer.\n",
    "        layers.append(Dense(PIXELS, activation='sigmoid'))\n",
    "\n",
    "        model = Sequential(layers)\n",
    "        model.compile(optimizer=config['optimizer'], loss='binary_crossentropy')\n",
    "        name = 'rnn__' + stringify(config)\n",
    "\n",
    "        models.append((model, name))\n",
    "\n",
    "    return models[:3]\n",
    "\n",
    "\n",
    "def generate_cnn_models(options):\n",
    "    '''Generates a list of compiled convolutional neural networks.'''\n",
    "    models = []\n",
    "\n",
    "    for config in generate_configs(options):\n",
    "        layers = []\n",
    "\n",
    "        # Convolutional and pooling layers.\n",
    "        for neurons in config['bottom_layers']:\n",
    "            layers.append(Conv2D(neurons, config['kernel_size'], activation='relu', padding='same'))\n",
    "            layers.append(MaxPooling2D(config['pool_size'], padding='same'))\n",
    "\n",
    "        # Output must be flattened.\n",
    "        layers.append(Flatten())\n",
    "\n",
    "        # Decoder.\n",
    "        for neurons in config['top_layers']:\n",
    "            layers.append(Dense(neurons, activation='relu'))\n",
    "\n",
    "        # Output layer.\n",
    "        layers.append(Dense(PIXELS, activation='sigmoid'))\n",
    "\n",
    "        model = Sequential(layers)\n",
    "        model.compile(optimizer='adamw', loss='binary_crossentropy')\n",
    "        name = 'cnn__' + stringify(config)\n",
    "\n",
    "        models.append((model, name))\n",
    "\n",
    "    return models[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    '''A small struct-like class for recording a model's performance.'''\n",
    "    name: str\n",
    "    score: float\n",
    "    latency: float\n",
    "\n",
    "\n",
    "def train(models, X_train, y_train, X_test, y_test):\n",
    "    '''Trains a set of models on the supplied data.'''\n",
    "    results = []\n",
    "\n",
    "    for model, name in tqdm(models, desc='Training Models'):\n",
    "        model_filepath = f'{TRAINING_DIRECTORY_NAME}/{name}.keras'\n",
    "\n",
    "        # Callbacks to help increase model performance.\n",
    "        checkpointer = ModelCheckpoint(filepath=model_filepath, verbose=0, save_best_only=True)\n",
    "        stopper = EarlyStopping(patience=10, verbose=1)\n",
    "\n",
    "        # Train the model.\n",
    "        model.fit(X_train, y_train, epochs=10, verbose=0, validation_data=(X_test, y_test), callbacks=[checkpointer, stopper])\n",
    "\n",
    "        # Swap to the best version for evaluation.\n",
    "        model.load_weights(model_filepath)\n",
    "\n",
    "        # Evaluate the model's performance.\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "        predictions = np.round(predictions)\n",
    "        score = metrics.f1_score(predictions, y_test, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # Measure the model's latency.\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(10):\n",
    "            _ = model.predict(X_train[i:i+1], verbose=0)\n",
    "\n",
    "        latency = (time.time() - start) / 10\n",
    "\n",
    "        results.append(Result(name, score, latency))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "column_titles = f'| {\"Model Rank\":^12} | {\"Model Name\":^36} | {\"F1-Score\":^14} | {\"Latency\":^11} |'\n",
    "separator = '=' * len(column_titles)\n",
    "\n",
    "\n",
    "def select_best_and_worst_results(results, metric, best_filepath):\n",
    "    '''Returns the best and worst models/results based on the given metric (score or latency).'''\n",
    "    results.sort(key=metric)\n",
    "    best_result = results[0]\n",
    "    worst_result = results[-1]\n",
    "\n",
    "    # Save the best model.\n",
    "    shutil.copy2(f'{TRAINING_DIRECTORY_NAME}/{best_result.name}.keras', best_filepath)\n",
    "\n",
    "    return best_result, worst_result\n",
    "\n",
    "\n",
    "def print_header(title):\n",
    "    '''Prints a formatted header for displaying results.'''\n",
    "    title = f'|{title.center(len(column_titles) - 2)}|'\n",
    "\n",
    "    for section in [separator, title, separator, column_titles, separator]:\n",
    "        print(section)\n",
    "\n",
    "\n",
    "def print_top_results(title, results, count):\n",
    "    '''Prints the results of the top models.'''\n",
    "    print_header(title)\n",
    "\n",
    "    for i, result in enumerate(results[:count]):\n",
    "        # Names that are too long must be truncated with '...'.\n",
    "        name = result.name[:33] + '...' if len(result.name) > 22 else result.name\n",
    "        print(f'| {i + 1:^12} | {name:^36} | {result.score:^14.5f} | {result.latency:^11.5f} |')\n",
    "\n",
    "\n",
    "def evaluate(results, best_model_filepaths, count=5):\n",
    "    '''Shows the best scoring and fastest latency results in a table.'''\n",
    "    os.makedirs(BEST_MODEL_DIRECTORY_PATH, exist_ok=True)\n",
    "\n",
    "    # Print best results by score.\n",
    "    score_metric = lambda result: -result.score\n",
    "    best_score_result, worst_score_result = select_best_and_worst_results(results, score_metric, best_model_filepaths[0])\n",
    "    score_range = best_score_result.score - worst_score_result.score\n",
    "    print_top_results('Best F1-Score', results, count)\n",
    "    print(f'| {\"-\":^12} | {\"F1-SCORE RANGE\":^36} | {score_range:^14.5f} | {\"-\":^11} |')\n",
    "\n",
    "    # Print best results by latency.\n",
    "    latency_metric = lambda result: result.latency\n",
    "    best_latency_result, worst_latency_result = select_best_and_worst_results(results, latency_metric, best_model_filepaths[1])\n",
    "    latency_range = worst_latency_result.latency - best_latency_result.latency\n",
    "\n",
    "    print_top_results('Best Latency ', results, count)\n",
    "    print(f'| {\"-\":^12} | {\"LATENCY RANGE\":^36} | {\"-\":^14} | {latency_range:^11.5f} |')\n",
    "\n",
    "    print(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = generate_dataset()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67817a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dense_options = {\n",
    "    'neurons': [[], [128], [128, 256], [128, 256, 512], [128, 256, 512, 1024], [256, 512, 1024, 2048]],\n",
    "    'activation': [None, 'relu', 'tanh', 'sigmoid'],\n",
    "    'optimizer': ['adam', 'adamw', 'sgd']\n",
    "}\n",
    "\n",
    "dense_models = generate_fcnn_models(dense_options)\n",
    "results = train(dense_models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30858a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(results, FCNN_BEST_FILEPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(SAMPLES, TIMESTEPS, FEATURES)\n",
    "y = y.reshape(SAMPLES, TIMESTEPS, PIXELS)[:, -1, :]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b665284",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_options = {\n",
    "    'bottom_layers': [[64], [64, 256], [128, 256, 512]],\n",
    "    'top_layers': [[], [128, 512], [1024], [4096]],\n",
    "    'dropout': [0.0, 0.1],\n",
    "    'optimizer': ['adam', 'adamw', 'sgd']\n",
    "}\n",
    "\n",
    "rnn_models = generate_rnn_models(rnn_options)\n",
    "results = train(rnn_models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fa5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(results, RNN_BEST_FILEPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(SAMPLES, TIMESTEPS, FEATURES, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_options = {\n",
    "    'bottom_layers': [[32], [64], [32, 64], [32, 64, 128]],\n",
    "    'top_layers': [[], [128, 512], [128, 512, 1024], [8096]],\n",
    "    'pool_size': [(2, 2), (3, 3), (5, 5)],\n",
    "    'kernel_size': [(3, 3), (5, 5)],\n",
    "}\n",
    "\n",
    "cnn_models = generate_cnn_models(cnn_options)\n",
    "results = train(cnn_models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f031b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(results, CNN_BEST_FILEPATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f758a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "try:\n",
    "    main()\n",
    "except Exception as e:\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
